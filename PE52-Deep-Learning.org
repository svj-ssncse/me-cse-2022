* <<<CP1334>>> DEEP LEARNING
:properties:
:author: B Senthil Kumar, M Saritha
:date: 29 June 2018
:end:

#+startup: showall

{{{credits}}}
|L|T|P|C|
|2|0|2|3|

** Course Objectives
- To understand the basics of deep neural networks
- To understand CNN and RNN architectures of deep neural networks
- To comprehend the advanced deep learning models
- To learn deep learning algorithms and their applications to solve real world problems

{{{unit}}}
|Unit I|Deep Networks Basics|6| 
Linear Algebra: Scalars -- Vectors -- Matrices and tensors;
Probability Distributions -- Gradient-based Optimization -- Machine
Learning Basics: Capacity -- Overfitting and underfitting --
Hyperparameters and validation sets -- Estimators -- Bias and variance
-- Stochastic gradient descent -- Challenges motivating deep learning;
Deep Networks: Deep feedforward networks

{{{unit}}}
|Unit II|Convolutional Neural Networks|6| 
Convolution Operation -- Sparse Interactions -- Parameter Sharing --
Equivariance -- Pooling -- Convolution Variants: Strided -- Tiled --
Transposed and dilated convolutions; CNN Learning: Nonlinearity
Functions -- Loss Functions -- Regularization -- Optimizers --
Gradient Computation -- CNN through Visualization

{{{unit}}}
|Unit III|Recurrent Neural Networks|6| 
Unfolding Graphs -- RNN Design Patterns: Acceptor -- Encoder --
Transducer; Gradient Computation -- Sequence Modeling Conditioned on
Contexts -- Bidirectional RNN -- Sequence to Sequence RNN -- Deep
Recurrent Networks -- Long Term Dependencies; Leaky Units: Skip connections and dropouts

{{{unit}}}
|Unit IV|Autoencoders and Generative Models|6| 
Autoencoders: Undercomplete autoencoders -- Regularized autoencoders
-- Stochastic encoders and decoders -- Learning with autoencoders;
Deep Generative Models: Variational autoencoders -- Generative adversial networks

{{{unit}}}
|Unit V|Transformer Models for Natural Language Processing|6|
The Encoder-Decoder Framework - Attention Mechanisms - Transfer Learning in NLP - Hugging Face Transformers - Transformer Applications - The Hugging Face Ecosystem ;  Transformer Anatomy: The Transformer Architecture - The Encoder - The Decoder; Summarization: GPT-2 – T5

\hfill *Total: 30*

** List of Experiments
1. XOR implementation using neural networks
2. Multi-class Classification deep neural networks
3. Digit recognition using CNN
4. Next word prediction using RNN
5. Image augmentation using GAN
6. Text summarization using T5

\hfill *Total: 15*

** Course Outcomes
After the completion of this course, students will be able to: 
- CO1: Explain the basics in deep neural networks (K2)
- CO2: Apply Convolution Neural Network for real-world problems in image processing (K3)
- CO3: Apply Recurrent Neural Network and its variants for text analysis (K3)
- CO4: Apply generative models for data augmentation (K3)
- CO5: Apply Transformer models for natural language processing (K3)

** References
1. Ian Goodfellow, Yoshua Bengio, Aaron Courville, ``Deep Learning'', MIT Press, 2016.
2. Lewis Tunstall, Leandro von Werra, Thomas Wolf, ``Natural Language Processing with Transformers'',  O'Reilly Media, Inc., February 2022.
3. Seth Weidman. ``Deep Learning from Scratch: Building with Python from First Principles'', ‘O'Reilly Media, Inc, September 2019.
4. Salman Khan, Hossein Rahmani, Syed Afaq Ali Shah, Mohammed Bennamoun, ``A Guide to Convolutional Neural Networks for Computer Vision'', Synthesis Lectures on Computer Vision, Morgan & Claypool publishers, 2018.
5. Yoav Goldberg, ``Neural Network Methods for Natural Language Processing'', Synthesis Lectures on Human Language Technologies, Morgan & Claypool publishers, 2017.
6. Santanu Pattanayak, ``Pro Deep Learning with TensorFlow: A Mathematical Approach to Advanced Artificial Intelligence in Python'', Apress, 2017.



